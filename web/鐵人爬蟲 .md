# IT爬蟲 
[TOC]
:::info
速度都是n^3 
:::


## 內層解析
:::warning
此處使用re 去對字句做搜尋找出相對應的class name& re.sub 去除文字內容
:::

### userinfo 作者資訊
:::info 找尋做這訊息
在原始碼中我們找到
![](https://i.imgur.com/1iYzRz1.png)
``` =html
<img src="https://member.ithome.com.tw/avatars/157379?s=ithelp" class="img-circle qa-header__info-avatar"/>

艾米
                        </a>
```
:::
:::success
我們的程式碼為
``` =python 
users = SoupIn.find_all(class_=re.compile("^img-circle qa-header__info-avatar"))[0].getText()
# 去除空格
users = re.sub(r"\s+", '', users)
users
```
:::
### title
:::info
標題
![](https://i.imgur.com/TEtnw5Q.png)
在網頁原始碼中搜尋
``` =html
<h2 class="qa-header__title">
    
    計算機概論 - 網路通訊與網際網路 Networking and the internet (上)
    </h2>
```
:::
:::success
我們的程式碼為

``` =python
title = SoupIn.find_all(class_=re.compile("^qa-header__title"))[0].getText()
# 去除空格
title = re.sub(r"\s+", '', title)
title
```
:::

### 發布時間
:::info
在網頁原始碼中搜尋
![](https://i.imgur.com/M2R2HTy.png)

``` =html
<span class="qa-header__info-view">3253 瀏覽</span>
```
:::
:::success
我們使用
``` =python
    info_time = SoupIn.find_all(class_=re.compile("^qa-header__info-time"))[0].getText()
    info_time = re.sub(r"\s+", '', info_time)
    info_time

```
:::danger
會有沒發布時間的狀況
 ``` =python
 #發布時間
 
    info_time =[]
    try: 
    

      info_time = SoupIn.find_all(class_=re.compile("^qa-header__info-time"))[0].getText()
      info_time = re.sub(r"\s+", '', info_time)
      info_time
    except:
      info_time

:::
### LINE數量
![](https://i.imgur.com/8iWR37z.png)
:::info
在網頁原始碼中搜尋
``` =html
<div class="likeGroup__num">4</div>

```
:::
:::success
我們使用
``` =python
   #LIKENUM
    likeGroup__num = SoupIn.find_all(class_=re.compile("^likeGroup__num"))[0].getText()
    likeGroup__num = re.sub(r"\s+", '', likeGroup__num)
    likeGroup__num
````
:::danger
因為今天有可能會有沒有like 情況所以我們在程式前`try`

``` =python
   #LIKENUM
   #假設一個空字串
   likeGroup__num =[]
   try :
    likeGroup__num = SoupIn.find_all(class_=re.compile("^likeGroup__num"))[0].getText()
    likeGroup__num = re.sub(r"\s+", '', likeGroup__num)
    likeGroup__num
   except IndexErro:
     likeGroup__num
````

:::
### 瀏覽次數
:::info
![](https://i.imgur.com/SuZpSQ2.png)
``` =html
<span class="qa-header__info-view">79 瀏覽</span>
```
:::
:::success
``` =python
 #瀏覽次數
    info_view = SoupIn.find_all(class_=re.compile("^qa-header__info-view"))[0].getText()
    info_view = re.sub(r"\s+", '', info_view)
    /*
    會有瀏覽
    XX 瀏覽 所以我們必須去除
    */
    info_view= re.sub(r"瀏覽+", '', info_view)
    info_view = int(info_view)
```
:::danger
因為今天有可能會有沒有瀏覽 情況所以我們在程式前`try`
``` =python
    info_view=[]
    try:
      info_view = SoupIn.find_all(class_=re.compile("^qa-header__info-view"))[0].getText()
      info_view = re.sub(r"\s+", '', info_view)
      info_view= re.sub(r"瀏覽+", '', info_view)
      info_view = int(info_view)
    except IndexError:
      info_view
```
:::
### 留言
:::info
![](https://i.imgur.com/HlGHNMp.png)

``` =html
<span class="qa-action__link-num">2</span>    
```
:::
:::success
``` =python
    #留言數量
    likeMESSEGE__num = SoupIn.find_all(class_=re.compile("^qa-action__link-num"))[0].getText()
    likeMESSEGE__num = re.sub(r"\s+", '', likeMESSEGE__num)
    likeMESSEGE__num
```
:::danger
因為今天有可能會有沒有留言 情況所以我們在程式前`try`
``` =python
     #留言數量
    likeMESSEGE__num = []
    try: 
    
      likeMESSEGE__num = SoupIn.find_all(class_=re.compile("^qa-action__link-num"))[0].getText()
      likeMESSEGE__num = re.sub(r"\s+", '', likeMESSEGE__num)
      likeMESSEGE__num

    except IndexError:
      likeMESSEGE__num
```
:::
### content內文
:::info
![](https://i.imgur.com/vihno9F.png)
``` =html
<div class="markdown__style">
                                                            <p>要考虑的主要和最重要的事情是熟悉不同特殊教育需求的行话和定义。成功的教学要求教师了解每个学生及其个人需求，无论是 SEN 还是 G&amp;T，以便有效地满足他们的教育需求。计划以及跟踪和监控进度、有效评估的能力都是一揽子计划的重要组成部分。虽然对于特殊需要学校也可以这样说，但需要意识到从主流向特殊需要的过渡可能会非常艰巨和令人沮丧，尤其是当对 SEN 陈述，特别是相关特征和有效应对方法的了解不足时这些。</p>
<p>SEN 需求范围很广，包括 ASD、SPLD、SLD、BESD、MLD、ADHD、SLCN、AD
```
:::
:::success
我們的程式碼為
``` =python 
cotent = SoupIn.find_all(class_=re.compile("^markdown__style"))[0].getText()
# 去除空格
cotent = re.sub(r"\s+", '', cotent)
cotent
```
:::
### response(所有留言)
#### response_USER
:::success
我們的程式碼為
``` =python 
response_USER = SoupIn.find_all(class_=re.compile("^comment__person"))[0].getText()
response_USER= re.sub(r"\s+", '', response_USER)
response_USER
```
:::
#### response_TEXT
:::success
``` =python 
response_TEXT = SoupIn.find_all(class_=re.compile("^response-markdown"))[0].getText()
response_TEXT= re.sub(r"\s+", '', response_TEXT)
response_TEXT 
```
:::
#### response_TIME
:::success
``` =python 
response_TIME= SoupIn.find_all(class_=re.compile("^comment__time"))[0].getText()
response_TIME = re.sub(r"\s+", '', response_TIME )
response_TIME
```
:::
## 所有的程式碼
### 除法器
:::success
函式 Quotient
:::
``` =python 
def finout(SearchNum):
  # Remainder
  if SearchNum%30 != 0:
    Quotient=SearchNum/30
    result=int(Quotient)+1
    
  if SearchNum%30 == 0:
    Quotient=SearchNum/30
    result=int(Quotient)
  return result
```
### url 搜尋器
:::success
函式  url 搜尋器
:::
``` =python
def url_finder(PageNum,SearchNum):
  
  all_ls=[]
  mergeLs = []
  for i in range ((PageNum)):
    url = f'https://ithelp.ithome.com.tw/articles?tab=tech&page{i+1}'
    print(url)
    html = req.get(url,headers=HEADERS)
    soup = BeautifulSoup(html.text,'html.parser')
    Qa_list = soup.find_all(attrs={"class": "qa-list"})
    Qa_list_url = [Qa_list[i].find_all(class_=re.compile("^qa-condition"))[0].get('href')for i in  range(len(Qa_list))]
    all_ls.append(Qa_list_url)
  for i in range(len(all_ls)):
    a=all_ls[i]
    for mc in a :
      mergeLs.append(mc)
  # 回傳我們要搜尋數量
  return mergeLs[0:SearchNum]
```
### 內容爬蟲機
:::success
json內容留言爬蟲機
:::
``` =python
def Qacrewing(QaListUrl):
  info_it=[]
  for i in QaListUrl:
    HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36"
}
    """
    有鑑於會出現找不到index 問題所以我這邊使用了相關
    """
    htmlin =req.get(i,headers=HEADERS)
    SoupIn = BeautifulSoup(htmlin.text,'html.parser')
    #作者
    users = SoupIn.find_all(class_=re.compile("^img-circle qa-header__info-avatar"))[0].getText()
    users = re.sub(r"\s+", '', users)
    users

    #TTILE
    title = SoupIn.find_all(class_=re.compile("^qa-header__title"))[0].getText()
    title = re.sub(r"\s+", '', title)
    title

    
        #瀏覽次數
    info_view=[]
    try: 
      info_view = SoupIn.find_all(class_=re.compile("^qa-header__info-view"))[0].getText()
      info_view = re.sub(r"\s+", '', info_view)
      info_view= re.sub(r"瀏覽+", '', info_view)
      info_view = int(info_view)
    except IndexErro :
      info_view
    #LIKENUM
    likeGroup__num=[]
    try:     
      likeGroup__num = SoupIn.find_all(class_=re.compile("^likeGroup__num"))[0].getText()
      likeGroup__num = re.sub(r"\s+", '', likeGroup__num)
      likeGroup__num
    except IndexErro :
      likeGroup__num
    #留言數量
    likeMESSEGE__num = []
    try: 
    
      likeMESSEGE__num = SoupIn.find_all(class_=re.compile("^qa-action__link-num"))[0].getText()
      likeMESSEGE__num = re.sub(r"\s+", '', likeMESSEGE__num)
      likeMESSEGE__num

    except IndexError:
      likeMESSEGE__num
    #發布時間
    info_time =[]
    try: 
    

      info_time = SoupIn.find_all(class_=re.compile("^qa-header__info-time"))[0].getText()
      info_time = re.sub(r"\s+", '', info_time)
      info_time
    except:
      info_time
    # 內文
    cotent = SoupIn.find_all(class_=re.compile("^markdown"))[0].getText()
    cotent = re.sub(r"\s+", '', cotent)
    cotent
    # 所有留言
    response_info= []
    response=SoupIn.find_all(class_=re.compile("^response-markdown"))
    for r in range (len(response)):
        # print("response_text",response_text)
        response_text = SoupIn.find_all(class_=re.compile("^response-markdown"))[r].getText()
        response_text = re.sub(r"\s+", '', response_text)
        response_text
        #留言時間
        response_TIME= SoupIn.find_all(class_=re.compile("^ans-header__time"))[r].getText()
        response_TIME = re.sub(r"\s+", '', response_TIME )
        response_TIME
        # 回應者
        response_USER = SoupIn.find_all(class_=re.compile("^response-header__person"))[r].getText()
        response_USER= re.sub(r"\s+", '', response_USER)
        response_USER
        response_info.append({
        "留言時間":response_TIME,
        "回應者":response_USER,
        "回應內容":response_text
        })
      
    info_it.append({
      "作者ID":users,
      "標題":title,
      "發布時間":info_time,
      "like次數":likeGroup__num, 
      "留言次數":likeMESSEGE__num,
      "瀏覽次數":info_view, 
      "內文":cotent,
      "所有留言":response_info})
  return info_it
 
```
:::success
主程式
:::

``` =python
if __name__ == '__main__':
    searchnum=input("請輸入搜尋:")
    searchnum = int(searchnum)
    alllst=finout(searchnum)
    allinone=url_finder(alllst,searchnum)
    jsonIT=Qacrewing(allinone,searchnum)
    
    #pandas 寫法
    df_all=pd.DataFrame(jsonIT)
    json_Name = '/content/NLP_LAB_Homework.json'
    with open(json_Name, 'w', encoding='utf-8') as file:
        df_all.to_json(file, force_ascii=False)
    
 
/*  
    # filewrite 寫法  
    with open(json_Name, "w") as f:  
        f.write(jsonIT)  
        f.close()  
        */
```
